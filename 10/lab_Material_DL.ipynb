{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Лабораторная работа 10. Матчасть DL\n",
    "\n",
    "Задача: реализовать и обучить нейронную сеть, состоящую из 2 нейронов, предсказывать значения функции XOR. При выполнении лабораторной запрещается использовать фреймворки для глубокого обучения (как PyTorch, Tensorflow, Caffe, Theano и им подобные).\n",
    "\n",
    "В первую очередь ознакомиться с [этим](https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d) материалом.\n",
    "\n",
    "Что необходимо реализовать, используя знания и фрагменты кода из ссылки выше:\n",
    "1. Класс Neuron, имеющий вектор весов self._weigths\n",
    "2. Два метода класса Neuron: forward(x), backward(x, loss) - реализующих прямой и обратный проход по нейронной сети.\n",
    "   Метод forward должен реализовывать логику работу нейрона: умножение входа на вес self._weigths, сложение и функцию активации сигмоиду.\n",
    "   Метод backward должен реализовывать взятие производной от сигмоиды и используя состояние нейрона обновить его веса.\n",
    "3. Реализовать с помощью класса Neuron нейронную сеть с архитектурой из трёх нейронов, предложенную в статье.\n",
    "\n",
    "    Для красоты обернуть в класс Model с методами forward и backward, реализующими правильное взаимодействие нейронов на прямом и обратном проходах.\n",
    "4. Реализовать тренировочный цикл следующего вида:\n",
    "\n",
    "```\n",
    "цикл (обучающие данные):\n",
    "\ty = model.forward(x)\n",
    "\terr = loss(y, label)\n",
    "\tmodel.backward(x, err)\n",
    "```\n",
    "В итоге обучения должны предсказываться значения аналогичные описанным в опорной статье.\n",
    "\n",
    "*Та же лаба человеческим языком:**\n",
    "Взять код из статьи, обернуть его в ООП. Попутно разобраться как работает прямое и обратное распространение."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Доп.материал](https://habr.com/ru/post/313216/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "def loss(predicted, expected):\n",
    "    return expected - predicted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_num, output_num, activ_func=sigmoid, loss_func=sigmoid_derivative, lr=0.1):\n",
    "        #Random weights and biases initialization\n",
    "        self.W = np.random.uniform(-1, 1, (input_num, output_num))  # Weights\n",
    "        self.b = np.random.uniform(-1, 1, (1, output_num))  # Biases\n",
    "        self.activ_func = activ_func\n",
    "        self.loss_func = loss_func\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.otput_value = self.activ_func(X @ self.W + self.b)  # sigma(XW+b)\n",
    "        return self.otput_value\n",
    "\n",
    "    def delta(self, error):\n",
    "        return error * self.loss_func(self.otput_value)  # delta = error * sigma'(XW+b)\n",
    "\n",
    "    def backward(self, x, delta):  # update weights and biases\n",
    "        self.W += x.T @ delta * self.lr\n",
    "        self.b += np.sum(delta, axis=0) * self.lr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, input_num, hidden_num, output_num, activ_func=sigmoid, loss_func=sigmoid_derivative, lr=0.1):\n",
    "        self.layer_h = Neuron(input_num, hidden_num, activ_func, loss_func, lr)  # Hidden Layer (2, 2)\n",
    "        self.layer_o = Neuron(hidden_num, output_num, activ_func, loss_func, lr)  # Output Layer (2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.output_h = self.layer_h.forward(x)\n",
    "        return self.layer_o.forward(self.output_h)\n",
    "\n",
    "    def backward(self, x, err):\n",
    "        delta_o = self.layer_o.delta(err)  # output neuron delta\n",
    "        self.layer_o.backward(self.output_h, delta_o)  # update weights of output neuron\n",
    "\n",
    "        error_h = delta_o.dot(self.layer_o.W.T)  # error at the output of hidden neurons\n",
    "        delta_h = self.layer_h.delta(error_h)  # hidden neurons delta\n",
    "        self.layer_h.backward(x, delta_h)  # update weights of hidden neurons"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "#Input datasets\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "expected_output = np.array([[0], [1], [1], [0]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial hidden weights: [-0.51801296 -0.43439121] [ 0.48013776 -0.4310847 ]\n",
      "Initial hidden biases: [-0.93001131  0.7195042 ]\n",
      "Initial output weights: [0.30274019] [0.90186948]\n",
      "Initial output biases: [0.42651689]\n",
      "Final hidden weights: [-5.56045937  4.55158174] [ 5.66032357 -4.88516238]\n",
      "Final hidden bias: [-3.32126841 -2.52868743]\n",
      "Final output weights: [7.06812462] [7.20454438]\n",
      "Final output bias: [-3.5162015]\n",
      "\n",
      "Output from neural network after 10,000 epochs: [0.0607929] [0.94953011] [0.94519298] [0.05439818]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "np.random.seed(19193)\n",
    "\n",
    "model = Model(2, 2, 1)\n",
    "\n",
    "print(\"Initial hidden weights: \",end='')\n",
    "print(*model.layer_h.W)\n",
    "print(\"Initial hidden biases: \",end='')\n",
    "print(*model.layer_h.b)\n",
    "print(\"Initial output weights: \",end='')\n",
    "print(*model.layer_o.W)\n",
    "print(\"Initial output biases: \",end='')\n",
    "print(*model.layer_o.b)\n",
    "\n",
    "#Training algorithm\n",
    "for _ in (range(epochs)):\n",
    "\tpredicted_output = model.forward(inputs)\n",
    "\terr = loss(predicted_output, expected_output)\n",
    "\tmodel.backward(inputs, err)\n",
    "\n",
    "print(\"Final hidden weights: \",end='')\n",
    "print(*model.layer_h.W)\n",
    "print(\"Final hidden bias: \",end='')\n",
    "print(*model.layer_h.b)\n",
    "print(\"Final output weights: \",end='')\n",
    "print(*model.layer_o.W)\n",
    "print(\"Final output bias: \",end='')\n",
    "print(*model.layer_o.b)\n",
    "\n",
    "print(\"\\nOutput from neural network after 10,000 epochs: \",end='')\n",
    "print(*predicted_output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Оригинальный код из статьи"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial hidden weights: [0.5488135  0.71518937] [0.60276338 0.54488318]\n",
      "Initial hidden biases: [0.4236548  0.64589411]\n",
      "Initial output weights: [0.43758721] [0.891773]\n",
      "Initial output biases: [0.96366276]\n",
      "Final hidden weights: [3.70986741 5.78826314] [3.71149043 5.79621929]\n",
      "Final hidden bias: [-5.6837777  -2.42016264]\n",
      "Final output weights: [-8.14923676] [7.50521163]\n",
      "Final output bias: [-3.37826206]\n",
      "\n",
      "Output from neural network after 10,000 epochs: [0.05770383] [0.9470198] [0.9469948] [0.05712647]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#Input datasets\n",
    "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "expected_output = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "epochs = 10000\n",
    "lr = 0.1\n",
    "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2,2,1\n",
    "\n",
    "#Random weights and bias initialization\n",
    "hidden_weights = np.random.uniform(size=(inputLayerNeurons,hiddenLayerNeurons))\n",
    "hidden_bias =np.random.uniform(size=(1,hiddenLayerNeurons))\n",
    "output_weights = np.random.uniform(size=(hiddenLayerNeurons,outputLayerNeurons))\n",
    "output_bias = np.random.uniform(size=(1,outputLayerNeurons))\n",
    "\n",
    "print(\"Initial hidden weights: \",end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Initial hidden biases: \",end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Initial output weights: \",end='')\n",
    "print(*output_weights)\n",
    "print(\"Initial output biases: \",end='')\n",
    "print(*output_bias)\n",
    "\n",
    "\n",
    "#Training algorithm\n",
    "for _ in range(epochs):\n",
    "\t#Forward Propagation\n",
    "\thidden_layer_activation = np.dot(inputs,hidden_weights)\n",
    "\thidden_layer_activation += hidden_bias\n",
    "\thidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "\toutput_layer_activation = np.dot(hidden_layer_output,output_weights)\n",
    "\toutput_layer_activation += output_bias\n",
    "\tpredicted_output = sigmoid(output_layer_activation)\n",
    "\n",
    "\t#Backpropagation\n",
    "\terror = expected_output - predicted_output\n",
    "\td_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "\n",
    "\terror_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
    "\td_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "\t#Updating Weights and Biases\n",
    "\toutput_weights += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
    "\toutput_bias += np.sum(d_predicted_output,axis=0,keepdims=True) * lr\n",
    "\thidden_weights += inputs.T.dot(d_hidden_layer) * lr\n",
    "\thidden_bias += np.sum(d_hidden_layer,axis=0,keepdims=True) * lr\n",
    "\n",
    "print(\"Final hidden weights: \",end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Final hidden bias: \",end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Final output weights: \",end='')\n",
    "print(*output_weights)\n",
    "print(\"Final output bias: \",end='')\n",
    "print(*output_bias)\n",
    "\n",
    "print(\"\\nOutput from neural network after 10,000 epochs: \",end='')\n",
    "print(*predicted_output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
